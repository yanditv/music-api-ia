# Bark Text-to-Speech API

Una API REST para sÃ­ntesis de voz usando el modelo Bark de Suno AI, optimizada para PyTorch 2.6+ con cache local persistente.

## ğŸš€ CaracterÃ­sticas

- **API REST completa** con FastAPI
- **Compatible con PyTorch 2.6+** (parche automÃ¡tico)
- **Cache local persistente** (modelos se descargan una sola vez)
- **Carga bajo demanda** (modelos se cargan solo cuando se necesitan)
- **MÃºltiples voces** (inglÃ©s, espaÃ±ol y otros idiomas)
- **Audio de calidad** (WAV 16-bit, 24kHz)

## ğŸ“‹ Requisitos

- Python 3.9+
- ~6.6GB de espacio libre (para modelos)
- ~3GB RAM (cuando modelos estÃ¡n cargados)

## ğŸ› ï¸ InstalaciÃ³n

1. **Clonar el repositorio**:

```bash
git clone <repo-url>
cd music-api-ia
```

2. **Crear entorno virtual**:

```bash
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# o
.venv\Scripts\activate     # Windows
```

3. **Instalar dependencias**:

```bash
pip install -r requirements.txt
```

## ğŸƒâ€â™‚ï¸ Uso

### Iniciar el servidor

**ğŸš€ Forma mÃ¡s simple:**

```bash
python start.py
```

**ğŸ”§ Otras formas:**

```bash
# Usando el mÃ³dulo app
python -m app

# Usando Make (si disponible)
make start

# Comando tradicional de uvicorn
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

La API estarÃ¡ disponible en: http://localhost:8000

### âš ï¸ Primera EjecuciÃ³n

**Importante**: La primera vez que uses la API:

1. **Los modelos se descargarÃ¡n automÃ¡ticamente al iniciar** (~6.6GB)
2. **El servidor tardarÃ¡ 1-2 minutos en estar listo** (descarga + carga)
3. **Una vez iniciado, todas las peticiones serÃ¡n rÃ¡pidas** (2-5 segundos)

**Ejemplo de primera ejecuciÃ³n**:

```bash
# 1. Iniciar servidor (LENTO - 1-2 minutos en primera vez)
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
# â³ AquÃ­ se descargan y cargan todos los modelos automÃ¡ticamente
# âœ… Cuando veas "Application startup complete", ya estÃ¡ listo

# 2. Primera peticiÃ³n (RÃPIDA - 2-5 segundos)
curl -X POST http://localhost:8000/generate/ \
  -H "Content-Type: application/json" \
  -d '{"text": "Hola mundo", "voice": "v2/es_speaker_0"}' \
  --output audio.wav

# 3. Segunda peticiÃ³n (TambiÃ©n RÃPIDA - 2-5 segundos)
curl -X POST http://localhost:8000/generate/ \
  -H "Content-Type: application/json" \
  -d '{"text": "Esta tambiÃ©n serÃ¡ rÃ¡pida", "voice": "v2/es_speaker_1"}' \
  --output audio2.wav
```

**Lo que sucede internamente**:

- ğŸ“¥ **Al iniciar**: Descarga modelos de Hugging Face â†’ `app/models/`
- ğŸ§  **Al iniciar**: Carga modelos en memoria (~3GB RAM)
- âœ… **Servidor listo**: Todas las peticiones son inmediatas
- âš¡ **Siguientes inicios**: Solo carga modelos (ya descargados), ~10-15 segundos

### DocumentaciÃ³n interactiva

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## ğŸŒ Endpoints

### `GET /health`

Verificar el estado de la API

```bash
curl http://localhost:8000/health
```

### `GET /voices`

Obtener lista de voces disponibles

```bash
curl http://localhost:8000/voices
```

### `POST /generate/`

Generar audio desde texto

```bash
curl -X POST http://localhost:8000/generate/ \
  -H "Content-Type: application/json" \
  -d '{"text": "Hola mundo", "voice": "v2/es_speaker_0"}' \
  --output audio.wav
```

**ParÃ¡metros**:

- `text` (string, requerido): Texto a convertir en audio
- `voice` (string, opcional): Voz a usar (por defecto: "v2/en_speaker_6")

## ğŸ­ Voces Disponibles

### InglÃ©s

- `v2/en_speaker_0` a `v2/en_speaker_9`

### EspaÃ±ol

- `v2/es_speaker_0` a `v2/es_speaker_9`

### Otros idiomas

- **Chino**: `v2/zh_speaker_0` a `v2/zh_speaker_2`
- **FrancÃ©s**: `v2/fr_speaker_0` a `v2/fr_speaker_2`
- **AlemÃ¡n**: `v2/de_speaker_0` a `v2/de_speaker_2`
- **Italiano**: `v2/it_speaker_0` a `v2/it_speaker_2`
- **JaponÃ©s**: `v2/ja_speaker_0` a `v2/ja_speaker_2`
- **Coreano**: `v2/ko_speaker_0` a `v2/ko_speaker_2`
- **PortuguÃ©s**: `v2/pt_speaker_0` a `v2/pt_speaker_2`
- **Ruso**: `v2/ru_speaker_0` a `v2/ru_speaker_2`
- Y mÃ¡s...

## ğŸ’¡ Ejemplos de Uso

### Python

```python
import requests

# Generar audio en espaÃ±ol
response = requests.post("http://localhost:8000/generate/",
    json={
        "text": "Â¡Hola! Esta es una prueba de sÃ­ntesis de voz en espaÃ±ol.",
        "voice": "v2/es_speaker_2"
    })

if response.status_code == 200:
    with open("audio_espaÃ±ol.wav", "wb") as f:
        f.write(response.content)
    print("Audio generado: audio_espaÃ±ol.wav")
```

### JavaScript

```javascript
fetch("http://localhost:8000/generate/", {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
  },
  body: JSON.stringify({
    text: "Hello, this is a test of Bark text-to-speech.",
    voice: "v2/en_speaker_3",
  }),
})
  .then((response) => response.blob())
  .then((blob) => {
    const url = window.URL.createObjectURL(blob);
    const a = document.createElement("a");
    a.href = url;
    a.download = "audio.wav";
    a.click();
  });
```

## ğŸ”§ ConfiguraciÃ³n TÃ©cnica

### Cache de Modelos

Los modelos se almacenan en `app/models/` y incluyen:

- `transformers/`: Modelos de transformers
- `huggingface/`: Cache de Hugging Face
- `torch/`: Modelos de PyTorch
- `cache/`: Cache general

### Variables de Entorno

Se configuran automÃ¡ticamente:

- `TRANSFORMERS_CACHE`
- `HF_HOME`
- `TORCH_HOME`
- `XDG_CACHE_HOME`

### Parche PyTorch 2.6+

Se aplica automÃ¡ticamente al importar `bark_utils`:

- Fuerza `weights_only=False` en `torch.load`
- Agrega `safe_globals` para numpy
- Compatible con numpy moderno y legacy

## ğŸ“ Estructura del Proyecto

```
music-api-ia/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py          # API FastAPI
â”‚   â”œâ”€â”€ bark_utils.py    # Funciones de Bark + parche PyTorch
â”‚   â””â”€â”€ models/          # Cache local de modelos (auto-creado)
â”œâ”€â”€ requirements.txt     # Dependencias Python
â”œâ”€â”€ Dockerfile          # Imagen Docker (opcional)
â”œâ”€â”€ README.md           # Este archivo
â””â”€â”€ STATUS.md          # Estado del proyecto
```

## ğŸ› SoluciÃ³n de Problemas

### â±ï¸ "El servidor tarda mucho en iniciar"

âœ… **Completamente normal** - En la primera ejecuciÃ³n:

- Se descargan ~6.6GB de modelos
- Se cargan en memoria (~3GB RAM)
- Puede tardar 1-2 minutos

**SoluciÃ³n**: Â¡Solo espera! Es una sola vez. Cuando veas "Application startup complete", ya estÃ¡ listo.

### ğŸ’¾ "Â¿DÃ³nde se guardan los modelos?"

Los modelos se almacenan localmente en:

```
app/models/
â”œâ”€â”€ transformers/    # Modelos de texto
â”œâ”€â”€ huggingface/     # Cache de HF
â”œâ”€â”€ torch/          # Modelos PyTorch
â””â”€â”€ cache/          # Cache general
```

Una vez descargados, **nunca se vuelven a descargar**.

### Error: "GLOBAL numpy.core.multiarray.scalar was not allowed"

âœ… **Solucionado automÃ¡ticamente** - El parche se aplica al importar la app.

### Error: "No module named 'bark'"

```bash
pip install git+https://github.com/suno-ai/bark.git
```

### Los modelos se descargan muy lento

- **Primera vez**: Es normal, son ~6.6GB
- **Siguientes**: Los modelos se cargan del cache local

### La API tarda mucho en responder

- **Primera vez que inicias**: Normal, estÃ¡ descargando y cargando modelos
- **Siguientes inicios**: ~10-15 segundos (carga modelos del cache)
- **Peticiones HTTP**: DeberÃ­an ser rÃ¡pidas (~2-5 segundos)

## ğŸ“Š Rendimiento

- **Primera carga**: 10-15 segundos (descarga + carga)
- **GeneraciÃ³n**: 2-5 segundos por frase
- **Memoria**: ~3GB cuando modelos estÃ¡n cargados
- **Almacenamiento**: ~6.6GB para todos los modelos

## ğŸš¢ Deployment

### Docker (opcional)

```bash
docker build -t bark-api .
docker run -p 8000:8000 bark-api
```

### ProducciÃ³n

- Considerar usar `gunicorn` en lugar de `uvicorn`
- Configurar reverse proxy (nginx)
- Implementar rate limiting
- Monitoreo y logs

## ğŸ“ Licencia

Este proyecto usa el modelo Bark de Suno AI. Consulta sus tÃ©rminos de uso.

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature
3. Commit tus cambios
4. Push a la rama
5. Abre un Pull Request

---

**ğŸ‰ Â¡Disfruta generando audio con Bark!**
